---
title: "Assignment 3 — Deployment & Integration of LLMs"
date: 2025-05-31
draft: false
tags: ["Machine Learning", "LLM", "API Integration", "Cloud Deployment", "Python", "VS Code"]
categories: ["Assignments"]
description: "Deploy one cloud-hosted and one local LLM, integrate both inside Visual Studio Code, and document the process end-to-end"
---

## 1 Objective

Deploy **one cloud‑hosted** and **one local** LLM, integrate both inside Visual Studio Code, and document the process end‑to‑end.

---

## 2 Environment Snapshot

| Component | Details                           |
|-----------|-----------------------------------|
| Host OS   | Windows 11 + WSL 2 (Ubuntu 22.04) |
| CPU / RAM | Intel i7‑12700H · 7.6 GiB RAM     |
| Editors   | VS Code 1.89 + Continue ext.      |
| Python    | 3.12 (venv `llm‐lab`)             |

---

## 3 Online Model API — Together AI

### 3.1 Key & Endpoint

```bash
pip install requests
export TOGETHER_AI_KEY="together‑xxxxxxxxxxxxxxxx"
Endpoint: https://api.together.xyz/v1/chat/completions
Model: mistralai/Mixtral‑8x7B‑Instruct‑v0.1

3.2 Minimal Test Script
pythonimport os, json, requests

url = "https://api.together.xyz/v1/chat/completions"
headers = {
    "Authorization": f"Bearer {os.getenv('TOGETHER_AI_KEY')}",
    "Content-Type": "application/json"
}
payload = {
    "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
    "messages": [
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Hello from Together AI!"}
    ]
}

response = requests.post(url, headers=headers, json=payload)
print(response.json()["choices"][0]["message"]["content"])
Show Image

Checkpoint: Cloud LLM replies, confirming key + endpoint work.


4 Local Model — Ollama llama3:8b
4.1 Installation and Setup
bash# One-time download (~4 GB)
ollama pull llama3:8b   

# Interactive chat
ollama run llama3:8b
Show Image

Checkpoint: Model answers fully offline.


5 IDE Integration — VS Code + Continue
5.1 Configuration
ProviderBase URLAuthTogether AIhttps://api.together.xyz/v1API keyOllama (LL3‑8B)http://127.0.0.1:11434none
5.2 Extension Setup
The Continue extension (v1.0.10) provides seamless integration with both cloud and local LLMs directly within VS Code.
Show Image

Note: Continue v1.0.10 currently sends OpenAI‑style calls to its global openai.* endpoint. Local model works perfectly; cloud calls show a 401 in this build. The standalone API (Section 3) verifies cloud access and will be re‑enabled once the extension supports custom base URLs.


6 Evaluation & Comparison
MetricTogether AIOllama llama3‑8BLatency (≈)1.8 s0.08 sCost / 1k tokensfree tier$0Max context32k8kInternet accessRequiredOfflinePrivacyCloud-basedFully local
Key Observations

Cloud model excels at large‑context reasoning and has access to broader knowledge
Local model provides instant responses, complete privacy, and zero ongoing costs
Hybrid approach allows leveraging strengths of both: cloud for complex reasoning, local for quick assistance


7 Practical Applications
7.1 Development Workflow Integration
The VS Code integration significantly improves development efficiency:

Code completion and suggestions
Documentation generation with proper docstrings
Refactoring assistance for cleaner code
Bug detection and resolution suggestions

7.2 Use Case Scenarios

Heavy reasoning tasks → Together AI (cloud)
Quick code assistance → Ollama (local)
Sensitive projects → Ollama (privacy-first)
Research and analysis → Together AI (broader knowledge)


8 Technical Challenges & Solutions
8.1 API Authentication

Secure key management using environment variables
Proper error handling for authentication failures

8.2 Performance Optimization

Local model provides near-instantaneous responses
Cloud model requires network optimization for best performance

8.3 Integration Limitations

Extension compatibility varies between versions
Custom endpoint configuration requires manual setup


9 Conclusion
The dual deployment approach successfully demonstrates the complementary nature of cloud and local LLMs:

Cloud LLMs provide powerful reasoning capabilities with extensive context windows
Local LLMs offer privacy, speed, and cost-effectiveness for routine tasks
VS Code integration streamlines the development workflow, reducing time spent on documentation and refactoring

This hybrid setup creates a robust AI-assisted development environment suitable for various project requirements and constraints.

10 Appendix
10.1 Dependencies

requirements.txt — requests, ollama‑py (optional)
Shell helpers: run_llama.sh, together_ping.py

10.2 Configuration Files

VS Code Continue extension settings
Environment variable setup scripts
API key management utilities

10.3 Future Enhancements

Automatic model switching based on task complexity
Enhanced privacy controls for sensitive code
Performance monitoring and optimization tools
